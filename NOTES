In spvm_all or spvm, must allow for kernels with double arguments. 
That means I need if statements. 

Now that I subclass, I get an error with the double precision. WHY? 
Problem with destructor

----------------------------------------------------------------------
June 10, 2013
Cascade: cases 7,9,10 run without error. 
         case 5 has errors.  <<<<<

Frodo (MIC): cases 7,9,10 run without error. 
         case 5 has errors <<<<

Gordo_mac (GPU): cases 7,9,10 run without error. 
         case 5 has no errors

case5: method0, method1 work (on mac)
case5: method0, method1 work (on cascade)
----------------------------------------------------------------------
Run different cases: 
Stencils: 10, 32, 64
Single and double precision
On K-20 and mic
Using SPMV (ell,bell,spell,sell), ViennaCL, my OpenCL
Perhaps try a OpenMP implementation.
Single versus double precision. 

Compare with FD and spectral.: 
  Perhaps cost per point? 
  GFLOP ==> percentage of peak

Based on nb workgroups size. 
with and without vectorization (add pragma: vec_hint<double4> on MIC)

Study cost of memory transfers on the different architectures (user perspective)
Study use when combined with using the derivatives (which incures more memory bandwidth)
----------------------------------------------------------------------
CREATE: 

Data structure that contains all run parameters: 

  - input file
  - kernel file/name
  - float or double
  - work group and total number of threads
  - nb nodes in template
  - grid size
  - type of process (MIC, K20, TESLA, AMD) 
  - type of CPU (how to get this)
  - register analysis of rows (NEED CODE FOR THIS)
  - sparsity type (COMPACT, RANDOM)
  - whether or not matrix data has been reordered and the type
     (Cut-Hill-McGee, space-filling curve)
  - sparse matrix format: (ELL, SELL, SBELL, BELL, CVR)
----------------------------------------------------------------------
No need to run all cases (bell, sell, etc.) for all matrices. Choose 1-2 representative, 
say 32 nodes per stencil.  Choose only large node configurations where GPU is saturated. 
Later on, do some spot checks. 

Generate the 3D datasets. 
----------------------------------------------------------------------
works if I read ascii file, does not work if read binary  file. 

I am getting buffer errors. DO NOT KNOW WHY. 
----------------------------------------------------------------------
July 2, 2013
What does the error message:  "too large" mean? 
output/random_x_weights_direct__no_hv_stsize_33_3d_64x_64y_64z.mtxb_case10:SBELL too large totalsize 8421376 bwidth 8 bheight 8
----------------------------------------------------------------------
July 19, 2013
Method 7: correct results if matrix constant, and vector constant. I also made matrix m = m*matrix(0), with correct results. 
Now create matrix with constant vertical lines. Column i: value: i. Then do row i: value i.

My generation of col_ids is incorrect. When randomized and when I have only 32 columns, and the 
matrix has only 32 columns. CHECK GENERATING code in rbffd. 
----------------------------------------------------------------------
June 22, 2013
Errors solved. 
method_7a: 19 gflops
method_8a: 18 gflops
----------------------------------------------------------------------
July 26, 2013
I get 100 Gflops with 4 functions and 4 matrices with method8a. And generate the correct results. Grids of 
64^3 and beyond. Time changes depending on cache size. 
Must order the points on a row from smallest to largest (not yet down) for better efficiency and to be consistent with 
real stencils. 

Implement cuthill McGee within this program using ViennaCL. 

Tests: Cuthill McGee, Center diagonal control, outer diagonal control.  Array size. 

To implement: double precision and its relation to cache size. 

Calculation of maximum possible speed on the intel based on theoretical bandwidth and estimated number of times
a function is accessed. As a function of number of zeros and their distribution. Benchmark based on fraction of speed
relative to maximum possible for a particular algorithm (as opposed to maximum speed of the machine.)
----------------------------------------------------------------------
July 26, 2013
64^3, random_diag. Vary bandwidth (no outside diagonals)
bandwidth, Gflops
1000, 100
3000, 85
5000, 62
7000, 49
9000, 42
11000, 37
13000, 35
31000, 25

Now take inner_bandwidth of 2000 and vary outer diagonal only, from 4000 to 20000. 
I get for 64^3, 88 Gflops, independent of diagonal separation. 
----------------------------------------------------------------------
July 27, 2013
- rewrite base version with one matrix and 1 vector
- rewrite version with four matrices and 1 vector
- rewrite version with one matrix and 1 vector

Do this in single and double precision. 

Modify inner_bandwidth, number of diagonals
On GPU: fastest index is down the column (columnwise ordering for col_id)
OpenMP CPU: rowise ordering is best.
----------------------------------------------------------------------
unroll use to have influence on spmv. Does it now? 
-O3 and -funroll-loops 
----------------------------------------------------------------------
----------------------------------------------------------------------
============== METHOD RD/WR LOCAL THREAD ===================
Using timestamp timing routines
nb threads: 1, tot_bw= 6.105246 Gbytes/sec, mean/max/min time per thread:  2.75/ 2.75/ 2.75 (ms)
nb threads: 2, tot_bw= 12.172888 Gbytes/sec, mean/max/min time per thread:  2.76/ 2.76/ 2.75 (ms)
nb threads: 4, tot_bw= 24.076849 Gbytes/sec, mean/max/min time per thread:  2.79/ 2.82/ 2.76 (ms)
nb threads: 8, tot_bw= 46.373275 Gbytes/sec, mean/max/min time per thread:  2.89/ 2.95/ 2.85 (ms)
nb threads: 16, tot_bw= 82.423080 Gbytes/sec, mean/max/min time per thread:  3.26/ 3.38/ 3.05 (ms)
nb threads: 32, tot_bw= 128.604299 Gbytes/sec, mean/max/min time per thread:  4.18/ 4.57/ 3.95 (ms)
nb threads: 64, tot_bw= 173.066414 Gbytes/sec, mean/max/min time per thread:  6.22/ 8.36/ 5.83 (ms)
nb threads: 96, tot_bw= 197.574192 Gbytes/sec, mean/max/min time per thread:  8.18/ 9.41/ 7.31 (ms)
nb threads: 128, tot_bw= 207.951850 Gbytes/sec, mean/max/min time per thread: 10.47/11.59/ 6.28 (ms)
nb threads: 160, tot_bw= 226.940644 Gbytes/sec, mean/max/min time per thread: 12.35/16.66/ 4.53 (ms)
nb threads: 192, tot_bw= 221.888762 Gbytes/sec, mean/max/min time per thread: 15.51/19.62/ 4.30 (ms)
nb threads: 224, tot_bw= 207.345530 Gbytes/sec, mean/max/min time per thread: 19.07/23.43/ 4.89 (ms)
nb threads: 244, tot_bw= 192.782415 Gbytes/sec, mean/max/min time per thread: 22.05/25.23/ 6.70 (ms)

Using Bollig timing routines (one per thread)
============== METHOD RD/WR LOCAL THREAD ===================
nb threads: 1, tot_bw= 6.056757 Gbytes/sec, mean/max/min time per thread:  2.77/ 2.77/ 2.77 (ms)
nb threads: 2, tot_bw= 11.199328 Gbytes/sec, mean/max/min time per thread:  3.00/ 3.14/ 2.86 (ms)
nb threads: 4, tot_bw= 23.785083 Gbytes/sec, mean/max/min time per thread:  2.82/ 2.88/ 2.73 (ms)
nb threads: 8, tot_bw= 45.813418 Gbytes/sec, mean/max/min time per thread:  2.93/ 3.07/ 2.82 (ms)
nb threads: 16, tot_bw= 84.057565 Gbytes/sec, mean/max/min time per thread:  3.19/ 3.34/ 3.06 (ms)
nb threads: 32, tot_bw= 130.285900 Gbytes/sec, mean/max/min time per thread:  4.13/ 4.44/ 3.86 (ms)
nb threads: 64, tot_bw= 174.191875 Gbytes/sec, mean/max/min time per thread:  6.17/ 6.62/ 5.90 (ms)
nb threads: 96, tot_bw= 199.341356 Gbytes/sec, mean/max/min time per thread:  8.37/ 9.64/ 3.94 (ms)
nb threads: 128, tot_bw= 180.347140 Gbytes/sec, mean/max/min time per thread: 12.30/15.37/ 5.06 (ms)
nb threads: 160, tot_bw= 175.823237 Gbytes/sec, mean/max/min time per thread: 16.83/22.95/ 4.16 (ms)
nb threads: 192, tot_bw= 145.818360 Gbytes/sec, mean/max/min time per thread: 25.69/32.13/ 4.09 (ms)
nb threads: 224, tot_bw= 149.611855 Gbytes/sec, mean/max/min time per thread: 31.29/43.96/ 4.30 (ms)
nb threads: 244, tot_bw= 125.267735 Gbytes/sec, mean/max/min time per thread: 38.41/51.28/ 6.57 (ms)


NOT CLEAR WHY RESULST SO CRUMMY. 
-
Reached 289 Gflops on 96 threds. Seems too good. 
nb threads: 1, tot_bw= 6.116375 Gbytes/sec, avg/max/min time per thread:  2.74/ 2.74/ 2.74 (ms)
nb threads: 2, tot_bw= 11.947276 Gbytes/sec, avg/max/min time per thread:  2.81/ 2.87/ 2.75 (ms)
nb threads: 4, tot_bw= 23.575794 Gbytes/sec, avg/max/min time per thread:  2.85/ 2.94/ 2.74 (ms)
nb threads: 8, tot_bw= 46.789210 Gbytes/sec, avg/max/min time per thread:  2.87/ 2.98/ 2.81 (ms)
nb threads: 16, tot_bw= 90.290370 Gbytes/sec, avg/max/min time per thread:  2.97/ 3.16/ 2.90 (ms)
nb threads: 32, tot_bw= 158.332026 Gbytes/sec, avg/max/min time per thread:  3.41/ 3.72/ 3.03 (ms)
nb threads: 64, tot_bw= 241.835555 Gbytes/sec, avg/max/min time per thread:  4.47/ 5.46/ 3.92 (ms)
nb threads: 96, tot_bw= 289.143710 Gbytes/sec, avg/max/min time per thread:  5.86/ 8.55/ 3.99 (ms)
nb threads: 128, tot_bw= 272.711227 Gbytes/sec, avg/max/min time per thread: 11.17/22.67/ 2.94 (ms)
nb threads: 160, tot_bw= 272.330875 Gbytes/sec, avg/max/min time per thread: 14.08/22.76/ 4.02 (ms)
nb threads: 192, tot_bw= 223.125175 Gbytes/sec, avg/max/min time per thread: 16.51/22.19/ 5.33 (ms)
nb threads: 224, tot_bw= 188.170859 Gbytes/sec, avg/max/min time per thread: 20.73/25.03/11.47 (ms)
nb threads: 244, tot_bw= 174.308685 Gbytes/sec, avg/max/min time per thread: 23.58/25.13/17.34 (ms)

How can I get to 700 Gbytes/sec? 
size= 0.524288 Mbytes
nb threads: 1, tot_bw= 9.709037 Gbytes/sec, avg/max/min time per thread:  0.11/ 0.11/ 0.11 (ms)
size= 0.524288 Mbytes
nb threads: 2, tot_bw= 19.889387 Gbytes/sec, avg/max/min time per thread:  0.11/ 0.11/ 0.10 (ms)
size= 0.524288 Mbytes
nb threads: 4, tot_bw= 39.146526 Gbytes/sec, avg/max/min time per thread:  0.11/ 0.11/ 0.10 (ms)
size= 0.524288 Mbytes
nb threads: 8, tot_bw= 79.806978 Gbytes/sec, avg/max/min time per thread:  0.11/ 0.11/ 0.10 (ms)
size= 0.524288 Mbytes
nb threads: 16, tot_bw= 160.586642 Gbytes/sec, avg/max/min time per thread:  0.10/ 0.11/ 0.10 (ms)
size= 0.524288 Mbytes
nb threads: 32, tot_bw= 313.285011 Gbytes/sec, avg/max/min time per thread:  0.11/ 0.12/ 0.10 (ms)
size= 0.524288 Mbytes
nb threads: 64, tot_bw= 586.886075 Gbytes/sec, avg/max/min time per thread:  0.12/ 0.19/ 0.10 (ms)
size= 0.524288 Mbytes
nb threads: 96, tot_bw= 728.971373 Gbytes/sec, avg/max/min time per thread:  0.15/ 0.26/ 0.10 (ms)
size= 0.524288 Mbytes
nb threads: 128, tot_bw= 619.778873 Gbytes/sec, avg/max/min time per thread:  0.26/ 0.44/ 0.11 (ms)
size= 0.524288 Mbytes
nb threads: 160, tot_bw= 553.717678 Gbytes/sec, avg/max/min time per thread:  0.43/ 0.97/ 0.11 (ms)
size= 0.524288 Mbytes
nb threads: 192, tot_bw= 529.078822 Gbytes/sec, avg/max/min time per thread:  0.50/ 0.69/ 0.12 (ms)
size= 0.524288 Mbytes
nb threads: 224, tot_bw= 579.728739 Gbytes/sec, avg/max/min time per thread:  0.58/ 0.84/ 0.12 (ms)
size= 0.524288 Mbytes
nb threads: 244, tot_bw= 418.467162 Gbytes/sec, avg/max/min time per thread:  0.79/ 1.66/ 0.14 (ms)

-
            __m512 v1_old = _mm512_load_ps(buf_orig + r);
            v1_old = _mm512_mul_ps(v1_old, v1_old);
            //_mm512_store_ps(buf_dest + r, v1_old);
            _mm512_storenrngo_ps(buf_dest+r, v1_old);

Adding _mul_ps (multiply), changes peak performance from 290 Gbytes/sec to 270 Gbytes/sec with 
96 threads. 

I really should control how many threads per core for better understanding. 
----------------------------------------------------------------------
